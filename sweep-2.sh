#!/bin/bash

# ================= 1. çŽ¯å¢ƒé…ç½®åŒº =================
# è¯·ç¡®ä¿ä½ çš„ conda çŽ¯å¢ƒåæ­£ç¡®

# æ¨¡åž‹å’Œæ•°æ®è·¯å¾„ (æ ¹æ®ä½ çš„å®žé™…æƒ…å†µä¿®æ”¹)
MODEL_PATH="/home/chenzhican/LLaMA-Factory/Llama-3-8B-Base/LLM-Research/Meta-Llama-3-8B"
BASE_OUTPUT_DIR="saves/llama3-8b/layer_scan_qlora"
SUMMARY_FILE="${BASE_OUTPUT_DIR}/scan_results.csv"

# æ‰«æçš„å±‚å·åˆ—è¡¨
# å»ºè®®æ‰«æ: 0(åº•å±‚), 4, 8, 12, 16(ä¸­å±‚), 20, 24, 28, 31(é¡¶å±‚)
LAYERS_TO_SCAN=( 0 4 8 12 16 20 24 28 31 )

# ================= 2. 16GB æ˜¾å­˜ä¼˜åŒ–å‚æ•° =================
# [æ˜¾å­˜æ•‘æ˜Ÿ] å¼€å¯ 4bit é‡åŒ–åŽï¼ŒRank 64 æ˜¯æ¯”è¾ƒå¹³è¡¡çš„é€‰æ‹©
# å¦‚æžœæ˜¾å­˜ä¾ç„¶ç´§å¼ ï¼Œå¯ä»¥å°† RANK é™ä¸º 32
RANK="32"
ALPHA="64"

# [è®­ç»ƒæ—¶é•¿] 200æ­¥è¶³å¤Ÿçœ‹æ¸… Loss ä¸‹é™è¶‹åŠ¿
STEPS="200"

# [æ˜¾å­˜æŽ§åˆ¶] å•å¡ BS=1ï¼Œç´¯ç§¯ 16 æ¬¡ => ç­‰æ•ˆ BS=16
BATCH_SIZE="1"
GRAD_ACCUM="16"

# [å­¦ä¹ çŽ‡] QLoRA é€šå¸¸éœ€è¦ç¨å¾®å¤§ä¸€ç‚¹çš„ LR
LR="2e-4"
# =======================================================

# åˆå§‹åŒ–ç»“æžœæ–‡ä»¶
mkdir -p $BASE_OUTPUT_DIR
echo "Layer_ID,Training_Loss,Eval_Loss" > $SUMMARY_FILE

echo "ðŸš€ Starting 4-bit QLoRA Layer Scan on 16GB GPU..."
echo "Target Layers: ${LAYERS_TO_SCAN[@]}"

for LAYER_ID in "${LAYERS_TO_SCAN[@]}"; do
    echo "----------------------------------------------------"
    echo "ðŸ§ª Processing Layer: $LAYER_ID"
    echo "----------------------------------------------------"

    # åŠ¨æ€æž„å»ºç›®æ ‡å±‚ (LocFFN æ¨¡å¼ï¼šåªå¾®è°ƒ MLP)
    # å¦‚æžœä½ æƒ³å¾®è°ƒè¯¥å±‚æ‰€æœ‰å‚æ•°(åŒ…æ‹¬Attention)ï¼Œè¯·åœ¨å­—ç¬¦ä¸²é‡ŒåŠ ä¸Š:
    # ,layers.${LAYER_ID}.self_attn.q_proj,layers.${LAYER_ID}.self_attn.v_proj
    TARGET="layers.${LAYER_ID}.mlp.gate_proj,layers.${LAYER_ID}.mlp.up_proj,layers.${LAYER_ID}.mlp.down_proj"
    
    OUTPUT_DIR="${BASE_OUTPUT_DIR}/layer_${LAYER_ID}"

    # å¯åŠ¨è®­ç»ƒ
    CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \
        --stage sft \
        --do_train \
        --do_eval \
        --model_name_or_path $MODEL_PATH \
        --template llama3 \
        --dataset alpaca_gpt4_en \
        --val_size 0.1 \
        --finetuning_type lora \
        --quantization_bit 4 \
        --lora_rank $RANK \
        --lora_alpha $ALPHA \
        --lora_target "$TARGET" \
        --output_dir $OUTPUT_DIR \
        --overwrite_output_dir \
        --per_device_train_batch_size $BATCH_SIZE \
        --gradient_accumulation_steps $GRAD_ACCUM \
        --learning_rate $LR \
        --lr_scheduler_type cosine \
        --warmup_ratio 0.05 \
        --max_steps $STEPS \
        --logging_steps 10 \
        --save_steps $STEPS \
        --save_total_limit 1 \
        --eval_steps $STEPS \
        --bf16 \
        --trust_remote_code true

    # ç»“æžœæå–é€»è¾‘ (è‡ªåŠ¨
    TRAIN_LOSS=$(python3 -c "import json; print(json.load(open('${OUTPUT_DIR}/trainer_state.json'))['log_history'][-2].get('loss', 'N/A'))" 2>/dev/null || echo "N/A")
    EVAL_LOSS=$(python3 -c "import json; print(json.load(open('${OUTPUT_DIR}/trainer_state.json'))['log_history'][-1].get('eval_loss', 'N/A'))" 2>/dev/null || echo "N/A")
    
    echo "${LAYER_ID},${TRAIN_LOSS},${EVAL_LOSS}" >> $SUMMARY_FILE
    
    echo "âœ… Layer $LAYER_ID Done. (Train: $TRAIN_LOSS | Eval: $EVAL_LOSS)"
done

echo "ðŸŽ‰ All Scans Completed! Results saved to: $SUMMARY_FILE"