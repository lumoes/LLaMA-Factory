#!/bin/bash

# ================= é…ç½®åŒº =================
source /root/miniconda3/etc/profile.d/conda.sh
conda activate v1

MODEL_PATH="/root/autodl-tmp/models/llama3-8b"
BASE_OUTPUT_DIR="saves/llama3-8b/layer_scan"
SUMMARY_FILE="${BASE_OUTPUT_DIR}/scan_results.csv"  # ç»“æžœæ±‡æ€»æ–‡ä»¶

# æ‰«æåˆ—è¡¨
LAYERS_TO_SCAN=( 0 4 8 10 12 16 20 24 28 31 )

LR="2e-4"           # ç¨å¾®åŠ å¤§ä¸€ç‚¹ï¼Œå› ä¸ºåªæœ‰200æ­¥ï¼Œè¦è®©å®ƒæ˜¾å½¢
RANK="256"
STEPS="300"         # ç¨å¾®åŠ é•¿ä¸€ç‚¹
# =========================================

# åˆå§‹åŒ–æ±‡æ€»æ–‡ä»¶å¤´
mkdir -p $BASE_OUTPUT_DIR
echo "Layer_ID,Training_Loss,Eval_Loss" > $SUMMARY_FILE

echo "ðŸš€ Starting Layer Sensitivity Scan..."

for LAYER_ID in "${LAYERS_TO_SCAN[@]}"; do
    echo "----------------------------------------------------"
    echo "ðŸ§ª Processing Layer: $LAYER_ID"
    echo "----------------------------------------------------"

    TARGET="layers.${LAYER_ID}.mlp.gate_proj,layers.${LAYER_ID}.mlp.up_proj,layers.${LAYER_ID}.mlp.down_proj"
    OUTPUT_DIR="${BASE_OUTPUT_DIR}/layer_${LAYER_ID}"

    # è®­ç»ƒ + è¯„ä¼°
    CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \
        --stage sft \
        --do_train \
        --do_eval \
        --model_name_or_path $MODEL_PATH \
        --template llama3 \
        --dataset math \
        --val_size 0.1 \
        --finetuning_type lora \
        --lora_rank $RANK \
        --lora_alpha 512 \
        --lora_target "$TARGET" \
        --output_dir $OUTPUT_DIR \
        --overwrite_output_dir \
        --per_device_train_batch_size 4 \
        --gradient_accumulation_steps 4 \
        --learning_rate $LR \
        --lr_scheduler_type cosine \
        --warmup_ratio 0.05 \
        --max_steps $STEPS \
        --logging_steps 10 \
        --save_steps $STEPS \
        --save_total_limit 1 \
        --eval_steps $STEPS \
        --bf16 \
        --trust_remote_code true

    # [å…³é”®] è‡ªåŠ¨æŠ“å–ç»“æžœ
    # ä»Ž trainer_state.json ä¸­æå–æœ€åŽçš„ loss (éœ€è¦ python æˆ– jqï¼Œè¿™é‡Œç”¨ç®€å•çš„ grep æå–é€»è¾‘)
    # å¦‚æžœæ²¡æœ‰ jqï¼Œæ‰‹åŠ¨çœ‹ä¹Ÿè¡Œï¼Œä½†å»ºè®®ç”¨ python one-liner æå–
    
    # è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•çš„ Python æå–è„šæœ¬
    TRAIN_LOSS=$(python3 -c "import json; print(json.load(open('${OUTPUT_DIR}/trainer_state.json'))['log_history'][-2]['loss'])" 2>/dev/null || echo "N/A")
    EVAL_LOSS=$(python3 -c "import json; print(json.load(open('${OUTPUT_DIR}/trainer_state.json'))['log_history'][-1]['eval_loss'])" 2>/dev/null || echo "N/A")
    
    echo "${LAYER_ID},${TRAIN_LOSS},${EVAL_LOSS}" >> $SUMMARY_FILE
    
    echo "âœ… Layer $LAYER_ID finished. Train Loss: $TRAIN_LOSS | Eval Loss: $EVAL_LOSS"
done

echo "ðŸŽ‰ All Scans Completed! Check results at: $SUMMARY_FILE"