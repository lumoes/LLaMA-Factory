### method
stage: sft
finetuning_type: ffn_only  # 再次确认你的代码逻辑对应这个名字
do_train: true

### dataset
dataset: math
cutoff_len: 4096
max_samples: 10000
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_num_workers: 4

### output
output_dir: /root/autodl-tmp/saves/math/456
logging_steps: 5          # 看得更细一点
save_steps: 100           # 约 1/3 epoch 存一次，防止崩盘
plot_loss: true
overwrite_output_dir: true
save_only_model: false
report_to: none 

### train
per_device_train_batch_size: 8    # 显存 48G 足够
gradient_accumulation_steps: 8    # ✅ 改动：让总 BS = 64，梯度更稳
learning_rate: 2e-5               # ✅ 改动：全参数微调特定层，安全起见降低 LR
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null

## eval
eval_dataset: math
val_size: 0.1
per_device_eval_batch_size: 8
eval_strategy: steps
eval_steps: 100      
load_best_model_at_end: true    # 训练结束后，自动加载效果最好的那个模型
metric_for_best_model: eval_loss # 根据验证集的 Loss 来评判
greater_is_better: false        # Loss 是越小越好
save_total_limit: 1             # 始终只保留那个“最好的”# ✅ 改动：配合 save_steps，频频看效果